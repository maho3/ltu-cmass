
save_dir: /anvil/scratch/x-nchartier/cmass/quijotelike_benchmark/preprocessed  # where to save trained models (default e.g. wdir/quijotelike/fastpm/models)

# ~~ GLOBAL ARGS ~~
exp_index: null  # index of experiment to run (null for all)
net_type: maf # if not None, net_index is ignored; careful about backend, eg "gf" does not work with sbi
N_type: 50 # we assume the "net_type"_"N_type".yaml exists (to change)
net_index: 386     # index of network to use in cfg.net
net_suffix: None # 15/02/2025, to separate my trainings by maf, gf, etc
device: cpu  # cpu or cuda

# ~~ EXPERIMENT ARGS ~~
Nmax: -1  # maximum number of simulations to load (-1 for all)
correct_shot: true  # whether to correct for shot noise when normalizing

halo: True      # whether to train on halo summaries
galaxy: False    # whether to train on galaxy summaries
ngc_lightcone: False # whether to train on ngc_lightcone summaries
sgc_lightcone: False # whether to train on sgc_lightcone summaries
mtng_lightcone: False # whether to train on mtng_lightcone summaries
simbig_lightcone: False # whether to train on simbig_lightcone summaries

experiments:
  - summary: [Pk0]   # monopole
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  - summary: [Pk0, Pk2, Pk4]   # monopole, quadrupole, hexadecapole
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [Pk0, Pk2]   # monopole, quadrupole, hexadecapole
  #   kmin: [0., 0.02]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [Pk0, Pk2, Pk4, Bk0]   # monopole, quadrupole, hexadecapole, reduced bispectrum
  #   kmin: [0.]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  - summary: [Pk0, Pk2, Pk4, Qk0]   # monopole, quadrupole, hexadecapole, reduced bispectrum
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [Pk0, Pk2, Pk4, EqBk0]   # monopole, quadrupole, hexadecapole, reduced bispectrum
  #   kmin: [0.,0.02]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [Pk0, Pk2, Pk4, EqQk0]   # monopole, quadrupole, hexadecapole, reduced equilateral bispectrum
  #   kmin: [0.,0.02]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  - summary: [zPk0]   # z-space monopole
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  - summary: [zPk0, zPk2]   # z-space monopole
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  - summary: [zPk0, zPk2, zPk4]  # z-space monopole, quadrupole, hexadecapole
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [zPk0, zPk2, zPk4, zBk0]   # monopole, quadrupole, hexadecapole, reduced bispectrum
  #   kmin: [0.]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  - summary: [zPk0, zPk2, zPk4, zQk0]   # monopole, quadrupole, hexadecapole, reduced bispectrum
    kmin: [0.,0.02]
    kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [zPk0, zPk2, zPk4, zEqBk0]   # monopole, quadrupole, hexadecapole, reduced bispectrum
  #   kmin: [0.,0.02]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]
  # - summary: [zPk0, zPk2, zPk4, zEqQk0]   # monopole, quadrupole, hexadecapole, reduced equilateral bispectrum
  #   kmin: [0.,0.02]
  #   kmax: [0.1, 0.2, 0.3, 0.4, 0.5]

# ~~ TRAINING ARGS ~~
Nnets: 10  # number of best networks to ensemble

val_frac: 0.1  # fraction of data to use for validation
test_frac: 0.1  # fraction of data to use for testing
seed: 0  # random seed for train/val/test split
only_cosmo: True  # only train on cosmological parameters (ignore HOD)

prior: quijote  # prior to use for training (uniform, quijote)
backend: lampe # backend to use for training (sbi/lampe/pydelfi)
engine: NPE    # engine to use for training (NPE/NLE/NRE)

batch_size: null  # batch size
learning_rate: null  # learning rate
stop_after_epochs: 50  # stop early after this many epochs
weight_decay: 0  # weight decay
lr_decay_factor: 0.5  # learning rate decay factor
lr_patience: 10  # learning rate decay patience


# ~~ OPTUNA ARGS ~~
hyperprior:
  model: ['nsf'] # , 'maf']
  hidden_features: [16, 512]
  num_transforms: [2, 8]
  fcn_width: [16, 512]
  fcn_depth: [0, 4]
  log2_batch_size: [5, 9]
  learning_rate: [5e-6, 1e-2]
  weight_decay: [1e-5, 1e-1]

n_trials: 25  # number of trials for hyperparameter optimization
n_startup_trials: 20  # number of startup trials for hyperparameter optimization